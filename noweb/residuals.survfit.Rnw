\section{Residuals for survival curves}
\subsection{R-code}
For all the more complex cases, the variance of a survival curve is based on 
the infinitesimal jackknife:
$$
D_i(t) = \frac{\partial S(t)}{w_i}
$$
evaluated at the the observed vector of weights.  The variance at a given 
time is then  $D'WD'$ where $D$ is a diagonal matrix of the case weights.
When there are multiple states $S$ is replaced by the vector $p(t)$, with
one element per state, and the formula gets a bit more complex.
The predicted curve from a Cox model is the most complex case.

Realizing that we need to return the matrix $D$ to the user, in order to compute
the variance of derived quantities like the restricted mean time in state, 
the code has been changed from a primarily internal focus (compute within the
survfit routine) to an external one. 

The underlying C code is very similar to that in survfitkm.c
One major difference in the routines is that this code is designed to return
values at a fixed set of time points; it is an error if the user does not
provide one.  This allows the result to be presented as a matrix or array.
Computational differences will be discussed later.

<<residuals.survfit>>=
# residuals for a survfit object
residuals.survfit <- function(object, times, type= "surv",
                              collapse=TRUE, weighted=FALSE, method=1, ...){
    if (!inherits(object, "survfit"))
        stop("argument must be a survfit object")
    survfitms <- inherits(object, "survfitms")
    coxsurv <- inherits(object, "survfitcox")
    timefix <- (is.null(object$timefix) || object$timefix)
    type <- match.arg(casefold(type), c("surv", "pstate", "cumhaz",
                                        "rmst", "rmts", "auc"))
    type.int <- c(1,1,3,2,2,2)[match(type, c("surv", "pstate", "cumhaz",
                                           "rmst", "rmts", "auc"))]
    
    start.time <- object$start.time
    if (is.null(start.time)) start.time <- min(c(0, object$time))

    # check input arguments
    if (missing(times)) 
        stop ("the times argument is required")
    else {
        if (!is.numeric(times)) stop("times must be a numeric vector")
        times <- sort(unique(times))
        if (timefix) times <- aeqSurv(Surv(times))[,1]
    }
    
    # get the data
    <<rsurvfit-data>>

    ny <- ncol(newY)
    if (collapse && any(X != X[1])) {
        # If the same id shows up in multiple curves, we just can't deal
        #  with it.
        temp <- unlist(lapply(split(cluster, X), unique))
        if (any(duplicated(temp)))
            stop("same id/cluster appears in multiple curves")
    }
    
    if (FALSE) {  # later consideration made me change this
    # order the data by cluster, which is the order we want for the output
    sort1 <- order(cluster)
    newY <- newY[sort1,]
    X <- X[sort1]
    casewt <- casewt[sort1]
    cluster <- cluster[sort1]
    }

    # What type of survival curve?
    if (!coxsurv) {
        stype <- Call$stype
        if (is.null(stype)) stype <- 1
        ctype <- Call$ctype
        if (is.null(ctype)) ctype <- 1
        
        if (!survfitms) {
            resid <- rsurvpart1(newY, X, casewt, times,
                                type.int, stype, ctype, object)
            dimnames(resid) <- list(NULL, times)
        }
        else {
            if (is.null(istate)) istate <-survcheck2(newY, id)$istate
            if (!collapse) cluster <- 1:nrow(Y)
            resid <- rsurvpart2(newY, X, casewt, istate, times, cluster,
                                type.int, object, method=method)
        }
    }
    else stop("coxph survival curves not yet available")
    
    if (weighted && any(casewt !=1)) resid <- resid*casewt
    if (collapse) {
        dd <- dim(resid)
        if (length(dd) ==3) {
            resid <- rowsum(matrix(resid, nrow=dd[1]), cluster, reorder=FALSE)
            dim(resid) <- c(length(resid)/(dd[2]*dd[3]), dd[2:3])
            dimnames(resid)[[1]] <- unique(cluster)
        }
        else {
            resid <- rowsum(resid, cluster, reorder=FALSE)
            rownames(resid) <- unique(cluster)
        }
    }
    resid
}
@ 

The first part of the work is retrieve the data set.  This is done in multiple
places in the survival code, all essentially the same.  
If I gave up (like lm) and forced the model frame to be saved this would be
easier of course.

<<rsurvfit-data>>=
# I always need the model frame
if (coxsurv) {
    mf <- model.frame(object)
    if (is.null(object$y)) Y <- model.response(mf)
    else Y <- object$y
}
else {
    Call <- object$call
    formula <- formula(object)

    # the chunk below is shared with survfit.formula 
    na.action <- getOption("na.action")
    if (is.character(na.action))
        na.action <- get(na.action)  # this is a temporary hack
    <<survfit.formula-getdata>>
    # end of shared code 
}

xlev <- levels(X)

# Deal with ties
if (is.null(Call$timefix) || Call$timefix) newY <- aeqSurv(Y) else newY <- Y

# Find the clustering, if any
if (!is.null(cluster)) {
    if (is.factor(cluster)) {
        clname <- levels(cluster)
        cluster <- as.integer(cluster)
    } else {
        clname  <- sort(unique(cluster))
        cluster <- match(cluster, clname)
    }
    ncluster <- length(clname)
} 
else if (!is.null(id)) {
    # treat the id as both identifier and clustering
    clname <- levels(id)
    cluster <- as.integer(id)
    ncluster <- length(clname)
}
else {
    # create our own clustering
    n <- nrow(Y)
    cluster <- 1:n
    ncluster <- n
    clname <- NULL
}   
@

The key trick in this calculation is to use a cumulative sum over the
survival times in the survfit object.
Say there were deaths at times (1, 24, 40), which occur in positions 10, 12,
and 17 in \code{fit\$time}, the Y observation of interest is (7,30), and
lsum=cumsum(...) is the cumulative sum.
We will want lsum[12] - lsum[10].
If there are multiple curves they are end to end as a single long vector,
adding a bit more nuisance to the compuatation.
The number of curves is normally modest.

\subsection{Simple survival}
<<residuals.survfit>>=
rsurvpart1 <- function(Y, X, casewt, times,
         type, stype, ctype, fit) {
     
    ntime <- length(times)
    etime <- (fit$n.event >0)
    ny <- ncol(Y)
    event <- (Y[,ny] >0)
    status <- Y[,ny]
    # 
    #  Create a list whose first element contains the location of
    #   the death times in curve 1, second element for curve 2, etc.
    #  row2 contains all the rows, censor and event
    #  
    if (is.null(fit$strata)) {
        fitrow <- list(which(etime))
        row2 <- list(seq(along=fit$time))
    }
    else {
        temp1 <- cumsum(fit$strata)
        temp2 <- c(1, temp1+1)
        fitrow <- lapply(1:length(fit$strata), function(i) {
            indx <- seq(temp2[i], temp1[i])
            indx[etime[indx]] # keep the death times
        })
        row2 <- lapply(1:length(fit$strata), function(i) seq(temp2[i],temp1[i]))
    }
 
    # for each time x, the index of the last death time which is <=x.
    #  0 if x is before the first death time
    matchfun <- function(x, fit, index) {
        dtime <- fit$time[index]  # subset to this curve
        i2 <- findInterval(x, dtime, left.open=FALSE)
        c(0, index)[i2 +1]
    }
     
    # output matrix D will have one row per observation, one col for each
    #  reporting time. tindex and yindex have the same dimension as D.
    # tindex points to the last death time in fit which
    #  is <= the reporting time.  (If there is only 1 curve, each col of
    #  tindex will be a repeat of the same value.)
    tindex <- matrix(0L, nrow(Y), length(times))
    for (i in 1:length(fitrow)) {
        yrow <- which(as.integer(X) ==i)
        temp <- matchfun(times, fit, fitrow[[i]])
        tindex[yrow, ] <- rep(temp, each= length(yrow))
    }

    # repeat the indexing for Y onto fit$time.  Each row of yindex points
    #  to the last row of fit with death time <= Y[,ny]
    ny <- ncol(Y)
    yindex <- matrix(0L, nrow(Y), length(times))
    event <- (Y[,ny] >0)
    if (ny==3) startindex <- yindex
    for (i in 1:length(fitrow)) {
        yrow <- (as.integer(X) ==i)  # rows of Y for this curve
        temp <- matchfun(Y[yrow,ny-1], fit, fitrow[[i]])
        yindex[yrow,] <- rep(temp, ncol(yindex))
        if (ny==3) {
            temp <- matchfun(Y[yrow,1], fit, fitrow[[i]])
            startindex[yrow,] <- rep(temp, ncol(yindex))
        }
    }                    

    # Now do the work
    if (type==3 || stype==2) {
        if (ctype==1) {
            <<residpart1-nelson>>
        } else {
            <<residpart1-fleming>>
        }
    } else {
        <<residpart1-AJ>>
    }
    D
}
@

\paragraph{Nelson-Aalen}
The Nelson-Aalen estimate of the
the influence at time $t$ for subject $i$ is
\begin{align}
  H(t) &= H(t-) + h(t) \nonumber \\
  \frac{\partial H(t)}{\partial w_i} &= \frac{\partial H(t-)}{\partial w_i} +
       [dN_i(t) - Y_i(t)h(t)]/r(t) \nonumber \\
       &= \sum_{d_j \le t} dN_i(d_j)/r(d_j) - Y_i(d_j)h(d_j)/r(d_j) 
         \label{NAderiv}
\end{align}
where $H$ the cumulative hazard, 
$h$ is the increment to the cumulative hazard, $Y_i$ is 1 when a
subject is at risk, $dN_i$ marks an event for the subject, $r$ is the
total weighted number at risk, and $d_j$ is the set of death times.

Let $D$ be the matrix of dfbeta residuals, with one column for each requested
outcome time and one row per observation.
For the first term in equation \eqref{NAderiv}, use the fact that a
given observation $i$ has either 0 (censored) or 1 event time.
It contributes a value of $1/r(d_j)$ to $D$ for any column where $t \ge t_i$, 
the event time of observation $i$.
For the second term, create a running sum of $h(d_j)/r(d_j)$.  The contribution
for observation $i$ will be the difference in this cumulative sum between
min(report, entry) and min(report, exit) time.
The first min is to deal with an obs which enters after the report time.

<<residpart1-nelson>>=
add1 <- (yindex <= tindex & rep(event, ntime))
lsum <- unlist(lapply(fitrow, function(i) 
             cumsum(fit$n.event[i]/fit$n.risk[i]^2)))
    
term1 <- c(0, 1/fit$n.risk)[ifelse(add1, 1+yindex, 1)]
term2 <- c(0, lsum)[1+pmin(yindex, tindex)]
if (ny==3) term3 <- c(0, lsum)[1 + pmin(startindex, tindex)]

if (ny==2) D <- matrix(term1 -  term2, ncol=ntime)
else       D <- matrix(term1 + term3 - term2, ncol=ntime)

# survival is exp(-H) so the derivative is a simple transform of D
if (type==1) D <- -D* c(1,fit$surv)[1+ tindex]

@ 

Type 2 = the RMST, which is the area under $S(t)$ up to a given time. 
The influence for S is -inflence(H) * S, so
the influence on the RMST segment from $d_j$ to $d_{j+1}$ will be 
$-{\rm influence}(H(d_j)) S(d_j) [d_{j+1}-d_j]$), where $d$ are the death times.
Let $A(s,t)$ be the area under the survival curve from $s$ to $t$, with the
convention that if $s \ge t$ then $A(s,t) =0$.  Let
$dH_{ij}$ be the influence of observation $i$ on the $j$th cumulative
 hazard $H$,
and $dh_{ij}$ the influence of observation $i$ on the increment of $H$ at $d_j$. 
Then we have
\begin{align*}
  \frac{\partial A(0,t)}{\partial w_i} &= \sum_j -dH_{ij} 
  A(d_j, t \wedge d_{j+1})  \\
  \sum_j \left(\sum_{k \le j}(-dh_{ik}\right) A(d_j, t \wedge d_{j+1}) \\
   \sum_{jk} \left(Y_i(d_k)h(d_k) - dN_i(d_k)\right)/r(t_k) 
          A(d_k\vee d_j, t \wedge d_{j+1}) \\
 &= \sum_{k}\left[Y_i(d_k)h(d_k) - dN_i(d_k))/r(t_k)\right]
   \left(\sum_j A(d_j\vee d_k, t \wedge d_{j+1}) \right) \\
&= \sum_{k} \frac{Y_i(d_k)h(d_k)- dN_i(d_k)}{/r(d_k)} A(d_k, t) \\
& =\sum_{k} \frac{Y_i(d_k)h(d_k)- dN_i(d_k)}{/r(d_k)} \left[A(0,t) - A(0,d_k)
     \right]
\end{align*}

What we must avoid is a sum over event times for each subject; that 
is essentially $O(n^2)$ and will fail for large data sets.
First recognize that if $d_k \ge t$ the contribution is zero.
Break this into three parts:
\begin{align*}
  1 &=\sum_{d_k\le t} \frac{Y_i(d_k)h(d_k)- dN_i(d_k)}{/r(d_k)} A(0,t) \\
  2 &=\sum_{d_k \le t} \frac{Y_i(d_k)h(d_k)}{/r(d_k)} A(0, d_k) \\
  3 &=\sum_{d_k \le t} \frac{ dN_i(d_k)}{/r(d_k)} A(0,d_k)
\end{align*}

The first term is the leverage of the CH at time $t$, times a constant.
Since there is only one death for a
given subject, pointed to by yindex, the third term is a single value for
each subject/reporting time pair.
The third requires another running sum lsum2.
When $t$ lies between two event times, then $AUC(0,t)$ requires adding on the
last interval width times the survival.

<<residpart1-nelson>>=
if (type==2) { # type 2 = AUC
    auc <- unlist(lapply(row2, function(i) {
        temp <- c(1, fit$surv[i])
        cumsum(temp[-length(temp)] * diff(c(0, fit$time[i])))
    }))  # each element of AUC has same length as the survival curve

    # overtime has a row for each subject and a col for each event time
    overtime <- (times[col(D)]- c(0,fit$time)[1+tindex]) # t - last event time
    auc2 <- c(0, auc)[1 + tindex]  + overtime* c(1,fit$surv)[1+tindex] # A(0, t)
    aterm1 <- -D * auc2
    
    lsum2 <- unlist(lapply(fitrow, function(i) 
             cumsum(auc[i]*fit$n.event[i]/fit$n.risk[i]^2)))
    aterm2 <- c(0, lsum2)[1 + pmin(yindex, tindex)]
    aterm3 <- c(0, auc/fit$n.risk[fit$n.event>0])[ifelse(add1, 1+yindex, 1)]

    D <- matrix(aterm1 + aterm3 - aterm2, ncol=ntime)
}
@ 

\paragraph{Fleming-Harrington}
For the Fleming-Harrington estimator the calculation at a tied time differs
slightly.
If there were 10 at risk and 3 tied events, the Nelson-Aalen has an increment
of 3/10, while the FH has an increment of (1/10 + 1/9 + 1/8).  The underlying
idea is that the true time values are continuous and we observe ties due to
coarsening of the data.  The derivative will have 3 terms as well.  In this
case the needed value cannot be pulled directly from the survfit object.
Computationally, the number of distinct times at which a tie occurs is normally
quite small and the for loop below will not be too expensive.

<<residpart1-fleming>>=
nevent <- fit$n.event/casewt[1]
if (any(casewt != casewt[1])) {
    # Have to reconstruct the number of obs with an event
    temp <- lapply(seq(along=levels(X)), function(i) {
        keep <- which(as.numeric(X) ==i)
        dtime <- Y[keep & status==1, ny-1]
        count <- table(dtime)
        nindx <- (fitrow[[i]])[fit$n.event[fitrow[[i]]] > 0]
        nevent[nindx] <- as.vector(count)
        })
}

risk2 <- fit$n.risk
ltemp <- fit$n.event/fit$n.risk^2
for (i in which(nevent>1)) {
    denom <- risk2[i] - fit$n.event*(0:(nevent[i]-1))/nevent[i]
    risk2[i] <- mean(1/denom) # multiplier for the event
    ltemp[i] <- fit$n.event* mean(1/denom^2)
}

add1 <- (yindex >= tindex & rep(event, ntime))
lsum <- unlist(lapply(fitrow, function(i) cumsum(ltemp[i])))
term1 <- c(0, 1/risk2)[ifelse(add1, 1+yindex, 1)]
term2 <- c(0, lsum)[1+pmin(yindex, tindex)]
if (ny==3) term3 <- c(0, lsum)[1 + startindex]

if (ny==2) D <- matrix(term1 - term2, ncol=ntime)
else D <- matrix(term1 + term3 - term2, ncol=ntime)

if (type==1) D <- -D* c(0,fit$surv)[1+ tindex]
else if (type==2) { #RMST
    auc <- unlist(lapply(row2, function(i) {
        temp <- c(1, fit$surv[i])
        cumsum(temp[-length(temp)] * diff(c(0, fit$time[i])))
    }))

    overtime <- (times[col(D)]- c(0,fit$time)[1+tindex]) # t - last event time
    auc2 <- c(0, auc)[1 + tindex]  + overtime* c(1,fit$surv)[1+tindex] # A(0, t)
    aterm1 <- -D * auc2
    
    lsum2 <- unlist(lapply(fitrow, function(i) 
             cumsum(auc[i]*ltemp)))
    aterm2 <- c(0, lsum2)[1 + pmin(yindex, tindex)]
    aterm3 <- c(0, auc/risk2)[ifelse(add1, 1+yindex, 1)]

    D <- matrix(aterm1 + aterm3 - aterm2, ncol=ntime)
}
@ 


\paragraph{Aalen-Johansen}
For the Kaplan-Meier (a special case of the Aalen-Johansen) the underlying
algorithm is multiplicative.
\begin{align}
    S(t) &= S(t-) [1 - h(t)]  \nonumber\\
         &= \prod_{d_j \le t} [1- h(d_j)] \\
    U_i(t) &= \frac{\partial S(t)}{\partial w_i}  \nonumber\\
           &= U_i(t-) [1- h(t)] - 
               S(t-)\frac{\partial h(t)}{\partial w_i} \label{sUkm} \\
           &= \sum_{d_j \le t} S(d_{j}-)\frac{\partial 1- h(d_j}{\partial w_i}
                   \left[\prod_{d_j < d_k <=t} (1 - h(d_k)) \right]
                 \label{sUkm2} \\
            &= \sum_{d_j \le t} S(d_{j}-)\frac{\partial 1- h(d_j}{\partial w_i}
                   \frac{1-h(d_j)}{1-h(d_j)} 
                   \left[\prod_{d_j < d_k <=t} (1 - h(d_k)) \right]
                 \nonumber \\
          &= \left(\prod_{d_j \le t} [1- h(d_j)] \right)
              \left[-\sum_{d_j\le t} \frac{dN_i(d_j) - Y_i(d_j)h(d_j)}{r(d_j)}
                   \frac{1}{1-h(d_j} \right] S(t) \label{ajfactor}
\end{align}
where $h$ is the increment in  Nelson-Aalen at each event time and 
$d$ are the event times.  
The derivative form \eqref{sUkm} is based on expanding the recursive definition
of the KM, and \eqref{sUkm2} on using the second definition and the product
rule for derivatives: each term is replaced, one by one, with it's derivative.
The final step add $1-h(d_j)$ to the numberator and denominator, which allows
all of the $1-h$ term to be gathered together as $S(t)$.
Our algorithm is based on the last equation \eqref{ajfactor}.
Exectution is very similar to the algorithm for $dH$.
One insight with respect to this final formula 
is that the steps of the KM are a little bit larger than the
steps of $\exp(-H)$, and terms in the sum of equation \eqref{ajfactor}
are a little larger than those in the Nelson-Aalen.

<<residpart1-AJ>>=
add1 <- (yindex <= tindex & rep(event, ntime))
# dtemp avoids 1/0.  (When this occurs the influence is 0, since
#  the curve has dropped to zero; and this avoids Inf-Inf in term1-term2).
dtemp <- ifelse(fit$n.risk==fit$n.event, 0, 1/(fit$n.risk- fit$n.event))
hsum <- unlist(lapply(fitrow, function(i) 
             cumsum(dtemp[i]*fit$n.event[i]/fit$n.risk[i])))
    
term1 <- c(0, dtemp)[ifelse(add1, 1+yindex, 1)]
term2 <- c(0, hsum)[1+pmin(yindex, tindex)]
if (ny==3) term3 <- c(0, hsum)[1 + startindex]

if (ny==2) D <- matrix(term1 -  term2, ncol=ntime)
else       D <- matrix(term1 + term3 - term2, ncol=ntime)

if (type==1) D <- -D* c(1,fit$surv)[1+ tindex]
else if (type==2){
    auc <- unlist(lapply(row2, function(i) {
        temp <- c(1, fit$surv[i])
        cumsum(temp[-length(temp)] * diff(c(0, fit$time[i])))
    }))

    overtime <- (times[col(D)]- c(0,fit$time)[1+tindex]) # t - last event time
    auc2 <- c(0, auc)[1 + tindex]  + overtime* c(1,fit$surv)[1+tindex] # A(0, t)
    aterm1 <- -D * auc2
    
    lsum2 <- unlist(lapply(fitrow, function(i) 
             cumsum(auc[i]*(dtemp[i]*fit$n.event[i]/fit$n.risk[i]))))
    aterm2 <- c(0, lsum2)[1 + pmin(yindex, tindex)]
    aterm3 <- c(0, auc/fit$n.risk)[ifelse(add1, 1+yindex, 1)]

    D <- matrix(aterm1 + aterm3 - aterm2, ncol=ntime)
}
@
 
Just as in the prior AUC, our influence on the survival is (something)*S,
so the influence on the AUC is (same something) * AUC.  
This means that the code for type 2 is nearly identical to what came before.

\subsection{Multi-state Aalen-Johansen estimate}
For multi-state models a correction for ties of similar spirit to the 
Efron approximation in a Cox model (the ctype=2 argument for \code{survfit})
is difficult: the 'right' answer depends on the study.
Thus the ctype argument is not present.  
Both stype 1 and 2 are feasible, but currently one \code{stype=1} is
supported.
This makes the code somewhat simpler, but this is more than offset by the 
multi-state nature.
With multiple states we also need to account for influence on the starting
state $p(0)$.

On thing that can make this code slow is data that has been divided into a
very large number of intervals, giving a large number of observations for
each cluster.  We first deal with that by collapsing adjacent observations.

<<residuals.survfit>>=
rsurvpart2 <- function(Y, X, casewt, istate, times, cluster, type, fit,
                       method) {
    ny <- ncol(Y)
    ntime <- length(times)
    nstate <- length(fit$states)
    
    # ensure that Y, istate, and fit all use the same set of states
    states <- fit$states
    if (!identical(attr(Y, "states"), fit$states)) {
        map <- match(attr(Y, "states"), fit$states)
        Y[,ny] <- c(0, map)[1+ Y[,ny]]    # 0 = censored
        attr(Y, "states") <- fit$states
    }
    if (is.character(istate)) istate <- factor(istate)
    if (is.factor(istate)) {
        if (!identical(levels(istate), fit$states)) {
            map <- match(levels(istate), fit$states)
            if (any(is.na(map))) stop ("invalid levels in istate")
            istate <- map[istate]
        }
    } # istate is numeric, we take what we get and hope it is right

    # collapse redundant rows in Y, for efficiency
    if (ny==3 && any(duplicated(cluster))) {
        ord <- order(cluster, X, istate, Y[,1])
        cfit <- .Call(Ccollapse, Y, X, istate, cluster, casewt, ord -1L) 
        if (nrow(cfit) < .8*length(X))  {
            # shrinking the data by 20 percent is worth it
            temp <- Y[ord,]
            Y <- cbind(temp[cfit[,1], 1], temp[cfit[2], 2:3])
            X <- X[cfit[,1]]
            istate <- istate[cfit[1,]]
            cluster <- cluster[cfit[1,]]
        }       
    }

    # Compute the initial leverage
    inf0 <- NULL
    if (is.null(fit$call$p0)) { #p0 was not supplied by the user
        inf0 <- matrix(0., nrow=nrow(Y), ncol=nstate)
        i0fun <- function(i, fit, inf0) {
            # reprise algorithm in survfitCI
            p0 <- fit$p0
            t0 <- fit$time[1]
            if (ny==2) at.zero <- which(as.numeric(X) ==i)
            else       
                at.zero <- which(as.numeric(X) ==i &
                          (Y[,1] < t0 & Y[,2] >= t0))
            for (j in 1:nstate) {
                inf0[at.zero, j] <- (ifelse(istate[at.zero]==states[j], 1, 0) -
                                     p0[j])/sum(casewt[at.zero])
            }
            inf0
        }

        if (is.null(fit$strata)) inf0 <- i0fun(1, fit, inf0)
        else for (i in 1:length(levels(X)))
            inf0 <- i0fun(i, fit[i], inf0)  # each iteration fills in some rows
    }
   
    fit <- survfit0(fit)  # package the initial state into the picture
    start.time <- fit$time[1]

    # This next block is identical to the one in rsurvpart1, more comments are
    #  there
    etime <- (rowSums(fit$n.event) >0)
    event <- (Y[,ny] >0)
    # 
    #  Create a list whose first element contains the location of
    #   the death times in curve 1, second element for curve 2, etc.
    #  
    if (is.null(fit$strata)) fitrow <- list(which(etime))
    else {
        temp1 <- cumsum(fit$strata)
        temp2 <- c(1, temp1+1)
        fitrow <- lapply(1:length(fit$strata), function(i) {
            indx <- seq(temp2[i], temp1[i])
            indx[etime[indx]] # keep the death times
        }) 
    }
 
     # for each time x, the index of the last death time which is <=x.
     #  0 if x is before the first death time
     matchfun <- function(x, fit, index) {
         dtime <- fit$time[index]  # subset to this curve
         i2 <- findInterval(x, dtime, left.open=FALSE)
         c(0, index)[i2 +1]
     }
     

     if (type==3) {
         <<residpart2CH>>
     } else {
         <<residpart2AJ>>
     }
     D
}
@ 

/paragraph{Nelson-Aalen}
The multi-state Nelson-Aalen estimate of the cumulative hazard at time $t$
is a vector with one element for each observed transition pair.  If there
were $k$ states there are potentially $k(k-1)$ transition pairs, though 
normally only a small number will occur in a given fit.  
We don't keep track of a transition from state $j$ to state $j$.
Let $r(t)$ be the weighted number at risk at time $t$, in each state.
When some subject makes a $j:k$ transition, the $j:k$ transition will
have an increment of $w_i/r_j(t)$. 
This is precisely the same increment as the ordinary Nelson estimate.
The only change then is that we loop over the set of possible transitions,
creating a large output object.

<<residpart2CH>>=
# output matrix D will have one row per observation, one col for each
#  reporting time. tindex and yindex have the same dimension as D.
# tindex points to the last death time in fit which
#  is <= the reporting time.  (If there is only 1 curve, each col of
#  tindex will be a repeat of the same value.)
tindex <- matrix(0L, nrow(Y), length(times))
for (i in 1:length(fitrow)) {
    yrow <- (as.integer(X) ==i)
    temp <- matchfun(times, fit, fitrow[[i]])
    tindex[yrow, ] <- rep(temp, each= length(yrow))
}

# repeat the indexing for Y onto fit$time.  Each row of yindex points
#  to the last row of fit with death time <= Y[,ny]
ny <- ncol(Y)
yindex <- matrix(0L, nrow(Y), length(times))
event <- (Y[,ny] >0)
if (ny==3) startindex <- yindex
for (i in 1:length(fitrow)) {
    yrow <- (as.integer(X) ==i)  # rows of Y for this curve
    temp <- matchfun(Y[yrow,ny-1], fit, fitrow[[i]])
    yindex[yrow,] <- rep(temp, ncol(yindex))
    if (ny==3) {
        temp <- matchfun(Y[yrow,1], fit, fitrow[[i]])
        startindex[yrow,] <- rep(temp, ncol(yindex))
    }
}                    

dstate <- Y[,ncol(Y)]
istate <- as.numeric(istate)
ntrans <- ncol(fit$cumhaz)  # the number of possible transitions
D <- array(0, dim=c(nrow(Y), ntime, ntrans))

scount <- table(istate[dstate!=0], dstate[dstate!=0]) # observed transitions
state1 <- row(scount)[scount>0]
state2 <- col(scount)[scount>0]
temp <- paste(state1, state2, sep='.')
if (!identical(temp, colnames(fit$cumhaz))) stop("setup error")

for (k in length(state1)) {
    e2 <- Y[,ny] == state2[k]
    add1 <- (yindex <= tindex & rep(e2, ntime))
    lsum <- unlist(lapply(fitrow, function(i) 
             cumsum(fit$n.event[i,k]/fit$n.risk[i,k]^2)))
    
    term1 <- c(0, 1/fit$n.risk[,k])[ifelse(add1, 1+yindex, 1)]
    term2 <- c(0, lsum)[1+pmin(yindex, tindex)]
    if (ny==3) term3 <- c(0, lsum)[1 + startindex]

    if (ny==2) D[,,k] <- matrix(term1 -  term2, ncol=ntime)
    else       D[,,k] <- matrix(term1 + term3 - term2, ncol=ntime)
}
@ 

\paragraph{Aalen-Johansen}
The multi-state AJ estimate is more complex.  Let $p(t)$ be the vector
of probability in state at time $t$.
Then
\begin{align}
  p(t) &= p(t-) H(t) \\
  \frac{partial p(t)}{\partial w_i} &= \frac{\partial p(t-)}{\partial w_i} H(t)
     +  p(t-) \frac{\partial H(t)}{\partial w_i} \nonumber\\
   &= U_i(t-) H(t) + p(t-) \frac{\partial H(t)}{\partial w_i} \label{ajresidx}\\
\end{align}

When we expand the left hand portion of \eqref{ajresidx} to include all 
observations it becomes simple matrix multiplication, not so with
the right hand portion.
Each individual subject $i$ has a subject-specific
nstate * nstate derivative matrix $dH$, which will be non-zero only for the 
state (row) $j$ that the subject occupies at time $t-$. 
The $j$th row of $p(t-) dH$ is added to each subject's derivative.

The transition matrix at time $t$ has elements
\begin{align*}
H(t)_{jk} &= \frac{\sum_i I{s_i(t-)=j, s_i(t)=k} w_i}{\sum_i I{s_i(t-)=j} w_i} \\
    &= \frac{\sum_i I{s_i(t-)=j, s_i(t)=k} w_i}{r_j(t)}
\end{align*}
The notation $s_i(t)$ is the state of observation $i$ at time $t$ and 
$r_j(t)$ the weighted number at risk.
Each observation at risk appears in only 1 row of $H(t)$. 
The sum of each row is 1, and if there are no transitions from state $j$ at
this time, said row $j$ is equal to the row of an identity matrix.

We can also write the transition matrix as $H = I +C$, the identity plus
a correction.
The derivative is incremented by $-C_{j.}/r_j(t)$ for each subject in state
$j$ at time $t-$, and by a further increment of  $1/r_j(t)$ and $-1/r_j(t)$ to
the $j$ and $kth$ positions, resectively, of any observation who transitioned
from $j$ to $k$.

If we evaluate equation \label{ajresidx} directly there will be 
$O(nk^2)$ operations at each death time for the matrix product, and another
$O(nk)$  to add in the new increment.  For a large data set $d$ is often
of the same order as $n$, which makes this an expensive calculation.
Collapsing can in theory occur at the same time as the addition
step, so that $U$ has one row per cluster rather than one per observation.
However, if the user's data set has the same subject in multiple curves this
leads to subtle indexing issues, so we currently don't do it in the C-code.

<<residpart2AJ>>=
if (method==1) {
    # Compute the result using the direct method, in C code
    # the routine is called separately for each curve, data in sorted order
    #
    if (ny==2) asort1 <- 0 else asort1 <- order(Y[,1], Y[,2]) -1L
    asort2 <- order(Y[,2]) -1L
    is1 <- as.integer(istate) -1L  # 0 based subscripts for C
    if (all(as.integer(X) ==1)) { # only one curve
        tfit <- .Call(Csurvfitresid, Y, asort1, asort2, is1, 
                      casewt, fit$pstate[1,], inf0, times, start.time, 
                      type==2)
        if (ntime==1) {if (type==2) D <- tfit[[2]] else D <- tfit[[1]]}
        else {
            if (type==2) D <- array(tfit[[2]], dim=c(nrow(Y), nstate, ntime))
            else         D <- array(tfit[[1]], dim=c(nrow(Y), nstate, ntime))
        }
    }
    else { # one curve at a time
        ix <- as.numeric(X)  # 1, 2, etc
        D <- array(0, dim=c(nrow(Y), nstate, ntime))
        for (curve in 1:max(ix)) {
            if (ny==2) a1 <- 0 else a1 <- asort1[ix[asort1] == curve] 
            a2 <- asort2[ix[asort2] ==curve] 
 
            # call with a subset of the sort vector
            j <- which(ix== curve)
            tfit <- .Call(Csurvfitresid, Y, a1, a2, is1, 
                          casewt, fit$pstate[i,]$pstate , inf0[j,], times, 
                          start.time, type==2)
            if (type==2) D[j,,] <- tfit[[2]] else D[j,,] <- tfit[[1]]
        }
    }                                    
}
else {
    <<residpart2AJ2>>
}
@   

Applying the product rule for derivatives to the expression
$p(t) = p(0) H(d_1) H(d_2) \ldots$, leads to a sum, where each $H$ term is
replaced, one by one, with its derivative:
\begin{align}
  U_i(t) &= \sum_{d_j \le t} \left( p(d_j-) \frac{\partial H(d_j)}{\partial w_i} 
    \prod_{d_j <d_k \le t} H(d_k) \right) \label{ajresidy}
\end{align}
We cannot insert an $H(d_j)/H(d_j)$ term and rearrange the equation so as to
factor out $p(t)$, as was done in the KM case, however,
since matrix products do not commute.

For fast computing, we must not update all $n$ observation's influence at 
each of the $d$ deaths. One approach is to precompute the transition matrices.
Consider for a moment a single reporting time $t$ such that 
$d_{9} \le t < d_{10}$, and the derivative of $H(d_5)$, and a subject who
transitions from state 2 to state 3 in a 4 state model.

Consider for a moment a single reporting time $t$ such that 
$d_{9} \le t < d_{10}$, and the contribution of $H(d_5)$, for a subject who
transitions from state 2 to state 3 in a 4 state model.
\begin{equation}
  \frac{partial H(d_5)}{\partial w_i} = 
    \left[(0,-1, 1, 0)/r(d_5) H(d_6)H(d_7)H(d_8)H(d_9) \right] -
    \left[C_{2.}(d_4)/r(d_5)H(d_6)H(d_6)H(d_7)H(d_8)H(d_9) \right]
\end{equation}
The above only looks at row 2 of the partial, since that is all that matters
for this subject.
Pre-compute the matrix products since it can be done efficiently
in reverse order
\begin{align*}
  W_1 & = \prod_{j=2}^{9} H(d_j)
  W_2 &=  \prod_{j=3}^{9} H(d_j)
  \vdots &= \vdots
  W_{8} &= H(d_{9})
  W_{9} &= I
\end{align*}
Compute the $k$ by $k$ matrix $A= (p(t_4)/r(d_5) C(d_5)W_5$, which 
will be fast since the matrices are $k$ by $k$.  
Everyone in state 2 at time $d_5$ will have row 2 of $A$ added to
their row of the influence at time $t$.  Those who experience an event at
time $d_5$ have an extra entry $pm 1$ term carried through in the same
way.
This has eliminated a lot of matrix multiplication, but we still have
$O(nk)$ additions performed at each death.  The matrix $A$ will have a 0 row
for any states without transitions, which can be used to advantage.
But, when the number of states is small $O(ndk)$ is not tremendously better
than $O(ndk^2)$, and the code is more complex.

The real gain is when the $C(d_j)$ part of the derivative, which is the
same for everyone, is kept as a running sum. Each observation then has a single
addition and subtraction when they enter and leave the risk set.
There is one serious complication, which is when an observation spans one
of the reporting times, e.g., an obs of (20, 80) with events at 30, 50, 70 and
a read out at 60: they get a partial total at 60 and final total at 80.
I was not able to come up with an efficient way to deal with this: keeping
track of who is and is not at currenty at risk is an $O(nd)$ job.  But there
is a trick: use the survSplit code to make sure no such observations exist!
The premise is again that there will be only a handful of reporting times.
(If a user asks for lots, slowness is their own fault.)

To compute the area under the curve replace the matrices $W$ with an alternate
set of matrices $V$.  Going back to our example of an event at time $d_5$
and reporting at $d_9$.  
Gathering the lead terms together as $B$, the influence of $H(d_5)$ 
on times 5--9 is $B$, $B H(d_6)$, $B H(d_6) H(d_7)$, $B H(d_6) H(d_7) H(d_8)$,
and $B H(d_6) H(d_7) H(d_8) H(d_9)$, and the influence on the AUC will be
$B (d_6-d_5)$, $B[(d_6-d_5) H(d_6)(d_7-d_6)]$, \ldots, giving the series
\begin{align*}
  V_1 & = (t_2-t_1) + H(d_2)V_2
  V_2 &=  (t_3-t_2) + H(d_3)V_3
  \vdots &= \vdots
  V_{8} &= (t_9-t_8) + H(d_9)V_9
  V_{9} &= 0
\end{align*}
In the prior formulas $V$ replaces $W$. 
Again, any issues with intervals that span one of the reporting times
has been solved by the data split.

<<residpart2AJ2>>=
Yold <- Y
utime  <- fit$time[fit$time <= max(times) & etime] # unique death times
ndeath <- length(utime)    # number of unique event times
delta <- diff(c(start.time, utime))

# Expand Y
if (ny==2) split <- .Call(Csurvsplit, rep(0., nrow(Y)), Y[,1], times)
else split <- .Call(Csurvsplit, Y[,1], Y[,2], times)
X <- X[split$row]
casewt <- casewt[split$row]
istate <- istate[split$row]
Y <- cbind(split$start, split$end, 
            ifelse(split$censor, 0, Y[split$row,ny]))
ny <- 3

# Create a vector containing the index of each end time into the fit object
yindex <- ystart <- double(nrow(Y))
for (i in 1:length(fitrow)) {
    yrow <- (as.integer(X) ==i)  # rows of Y for this curve
    yindex[yrow] <- matchfun(Y[yrow, 2], fit, fitrow[[i]])
    ystart[yrow] <- matchfun(Y[yrow, 1], fit, fitrow[[i]])
}
# And one indexing the reporting times into fit
tindex <- matrix(0L, nrow=length(fitrow), ncol=ntime)
for (i in 1:length(fitrow)) {
    tindex[i,] <- matchfun(times, fit, fitrow[[i]])
}

# Create the array of C matrices
cmat <- array(0, dim=c(nstate, nstate, ndeath)) # max(i2) = ndeath, by design
Hmat <- cmat

# We only care about observations that had a transition; any transitions
#  after the last reporting time are not relevant
transition <- (Y[,ny] !=0 & Y[,ny] != istate &
               Y[,ny-1] <= max(times)) # obs that had a transition
i2 <- match(yindex, sort(unique(yindex)))  # which C matrix this obs goes to
i2 <- i2[transition]
from <- as.numeric(istate[transition])  # from this state
to   <- Y[transition, ny]   # to this state
nrisk <- fit$n.risk[cbind(yindex[transition], from)]  # number at risk
wt <- casewt[transition]
for (i in seq(along=from)) {
    j <- c(from[i], to[i])
    haz <- wt[i]/nrisk[i]
    cmat[from[i], j, i2[i]] <- cmat[from[i], j, i2[i]] + c(-haz, haz)
}
for (i in 1:ndeath) Hmat[,,i] <- cmat[,,i] + diag(nstate)

# The transformation matrix H(t) at time t  is cmat[,,t] + I
# Create the set of W and V matrices.
# 
dindex <- which(etime & fit$time <= max(times))
Wmat <- Vmat <- array(0, dim=c(nstate, nstate, ndeath))
for (i in ndeath:1) {
    j <- match(dindex[i], tindex, nomatch=0) 
    if (j > 0) {
        # this death matches one of the reporting times
        Wmat[,,i] <- diag(nstate)
        Vmat[,,i] <- matrix(0, nstate, nstate)
    } 
    else {
        Wmat[,,i] <- Hmat[,,i+1] %*% Wmat[,,i+1]
        Vmat[,,i] <- delta[i] +  Hmat[,,i+1] %*% Wmat[,,i+1]
    }
}
@

The above code has created the Wmat array for all reporting times and
for all the curves (if more than one). 
Each of them reaches forward to the next reporting time.
Now work forward in time.

<<residpart2AJ2>>=
iterm <- array(0, dim=c(nstate, nstate, ndeath)) # term in equation
itemp <- vtemp <- matrix(0, nstate, nstate)  # cumulative sum, temporary
isum  <- isum2 <- iterm  # cumulative sum
vsum  <- vsum2 <- vterm <- iterm
for (i in 1:ndeath) {
    j <- dindex[i]
    n0 <- ifelse(fit$n.risk[j,] ==0, 1, fit$n.risk[j,]) # avoid 0/0
    iterm[,,i] <- ((fit$pstate[j-1,]/n0) * cmat[,,i]) %*% Wmat[,,i]
    vterm[,,i] <- ((fit$pstate[j-1,]/n0) * cmat[,,i]) %*% Vmat[,,i]
    itemp <- itemp + iterm[,,i]
    vtemp <- vtemp + vterm[,,i]
    isum[,,i] <- itemp
    vsum[,,i] <- vtemp
    j <- match(dindex[i], tindex, nomatch=0)
    if (j>0) itemp <- vtemp <- matrix(0, nstate, nstate)  # reset
    isum2[,,i] <- itemp
    vsum2[,,i] <- vtemp
}

# We want to add isum[state,, entry time] - isum[state,, exit time] for
#  each subject, and for those with an a:b transition there will be an 
#  additional vector with -1, 1 in the a and b position.
i1 <- match(ystart, sort(unique(yindex)), nomatch=0) # start at 0 gives 0
i2 <- match(yindex, sort(unique(yindex)))
D <- matrix(0., nrow(Y), nstate)
keep <- (Y[,2] <= max(times))  # any intervals after the last reporting time
                                # will have 0 influence
for (i in which(keep)) {
    if (Y[i,3] !=0 && istate[i] != Y[i,3]) {
        z <- fit$pstate[yindex[i]-1, istate[i]]/fit$n.risk[yindex[i], istate[i]]
        temp <- double(nstate)
        temp[istate[i]] = -z
        temp[Y[i,3]]    =  z
        temp <- temp %*% Wmat[,,i2[i]] - isum[istate[i],,i2[i]]
        if (i1[i] >0) temp <- temp + isum2[istate[i],, i1[i]]
        D[i,] <- temp
    }
    else {
        if (i1[i] >0) D[i,] = isum2[istate[i],,i1[i]] - isum[istate[i],, i2[i]]
        else  D[i,] =  -isum[istate[i],, i2[i]]
    }
}
@

By design, each row of $Y$, and hence each row of $D$, corresponds to a unique
curve, and also to a unique period in the reporting intervals.
(Any Y intervals after the last reporting time will have D=0 for the row.)
If there are multiple reporting intervals, create an array with one
n by nstate slice for each.
If a row lies in the first interval, $D$ currently contains its influence
on that interval.  It's influence on the second interval is the vector times
$\prod H(d_k)$ where $k$ is the set of event times $>$ the first reporting time
and $\le$ the second one.  
 
<<residpart2AJ2>>=
Dsave <- D
if (!is.null(inf0)) {
    # add in the initial influence, to the first row of each obs
    #   (inf0 was created on unsplit data)
    j <- which(!duplicated(split$row))
    D[j,] <- D[j,] + (inf0%*% Hmat[,,1] %*% Wmat[,,1])
}
if (ntime > 1) {
    interval <- findInterval(yindex, tindex, left.open=TRUE)
    D2 <- array(0., dim=c(dim(D), ntime))
    D2[interval==0,,1] <- D[interval==0,]
    for (i in 1:(ntime-1)) {
        D2[interval==i,,i+1] = D[interval==i,]
        j <- tindex[i]
        D2[,,i+1] = D2[,,i+1] + D2[,,i] %*% (Hmat[,,j] %*% Wmat[,,j])
    } 
    D <- D2
}

# undo any artificial split
if (any(duplicated(split$row))) {
    if (ntime==1) D <- rowsum(D, split$row)
    else {
        # rowsums has to be fooled
        temp <- rowsum(matrix(D, ncol=(nstate*ntime)), split$row)
        # then undo it
        D <- array(temp, dim=c(nrow(temp), nstate, ntime))
    }
}
D
@
